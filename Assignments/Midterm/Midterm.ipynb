{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Xia\n",
    "\n",
    "jx643\n",
    "\n",
    "## Problem 1\n",
    "### part a\n",
    "\n",
    "State Space S={$(a_1,b_1),....(a_j,b_j)$} where maze_grid[$a_i,b_i$] is either \"SPACE\" or \"GOAL\"\n",
    "\n",
    "Terminating Space T={(a,b)} where maze_grid[a,b]=\"GOAL\"\n",
    "\n",
    "Action Space={moving up,moving down, moving left, moving right} which result to change of state accoridngly\n",
    "\n",
    "\n",
    "State Transition Function is that with each action the probability of moving to the next state is 1.0\n",
    "\n",
    "Reward Transition Function 1:\n",
    "\n",
    "Each step will have zero reward, recieve 1.0 reward if moving to the terminating state. Discount factor is between 1 and 0.\n",
    "\n",
    "Reward Transition Function 2:\n",
    "\n",
    "Each step wil have a negative reward of -1, and arraiving at the terminating state will have a positive reward of 100. Discount factor is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iteration to convergence for the first reward type is 17\n",
      "number of steps to finish maze is 16\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict,Mapping, Iterator\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "from rl.dynamic_programming import value_iteration_result,value_iteration,almost_equal_vfs\n",
    "from rl.iterate import converged, iterate,converge\n",
    "import math\n",
    "\n",
    "##Define the stateclass\n",
    "@dataclass(frozen=True)\n",
    "class MazeState:\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "# 0 is up, 1 is down, 2 is left,3 is right\n",
    "MazeStateMapping = StateActionMapping[MazeState, int]\n",
    "\n",
    "\n",
    "class SimpleMazeMDP(FiniteMarkovDecisionProcess[MazeState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        maze: Mapping[Tuple[int, int], str],\n",
    "        reward_type:int\n",
    "    ):\n",
    "        self.maze: Mapping[Tuple[int, int], str] = maze\n",
    "        self.reward_type=reward_type\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "    \n",
    "    ##Check if an action is valid\n",
    "    def checkAction(self,state:MazeState,action:int):\n",
    "        currentx=state.x\n",
    "        currenty=state.y\n",
    "        #check up\n",
    "        if(action==0):\n",
    "            if((currentx,currenty-1) in self.maze.keys()):\n",
    "                return self.maze[(currentx,currenty-1)]==\"SPACE\" or self.maze[(currentx,currenty-1)]==\"GOAL\"\n",
    "            else:\n",
    "                return False\n",
    "        #check down\n",
    "        if(action==1):\n",
    "            if((currentx,currenty+1) in self.maze.keys()):\n",
    "                return self.maze[(currentx,currenty+1)]==\"SPACE\" or self.maze[(currentx,currenty+1)]==\"GOAL\"\n",
    "            else:\n",
    "                return False\n",
    "        #check left\n",
    "        if(action==2):\n",
    "            if((currentx-1,currenty) in self.maze.keys()):\n",
    "                return self.maze[(currentx-1,currenty)]==\"SPACE\" or self.maze[(currentx-1,currenty)]==\"GOAL\"\n",
    "            else:\n",
    "                return False\n",
    "        #check right\n",
    "        if((currentx+1,currenty) in self.maze.keys()):\n",
    "            return self.maze[(currentx+1,currenty)]==\"SPACE\" or self.maze[(currentx+1,currenty)]==\"GOAL\"\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    ## take action and return the new maze state\n",
    "    def takeAction(self,state:MazeState,action:int):\n",
    "        if(action==0):\n",
    "            return MazeState(state.x,state.y-1)\n",
    "        \n",
    "        if(action==1):\n",
    "            return MazeState(state.x,state.y+1)\n",
    "        \n",
    "        if(action==2):\n",
    "            return MazeState(state.x-1,state.y)\n",
    "        return MazeState(state.x+1,state.y)\n",
    "    \n",
    "    ## Check if the state is at the terminal state\n",
    "    def checkGoal(self,state:MazeState):\n",
    "        return self.maze[(state.x,state.y)]==\"GOAL\"\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> MazeStateMapping:\n",
    "        d: Dict[MazeState, Dict[int, Categorical[Tuple[MazeState,float]]]] = {}\n",
    "        for pos in self.maze.keys():\n",
    "            #iterate through all the non-Block state\n",
    "            if(self.maze[pos]!=\"BLOCK\"):\n",
    "                state: MazeState=MazeState(pos[0],pos[1])\n",
    "                # terminal state returns None for the mapping\n",
    "                if(self.checkGoal(state)):\n",
    "                    d[state]=None\n",
    "                else:\n",
    "                    d1:Dict[int, Categorical[Tuple[MazeState,float]]]={}\n",
    "                    for action in range(4):\n",
    "                        sr_probs_dict: Dict[Tuple[MazeState, float], float]={}\n",
    "                        if(self.checkAction(state,action)):\n",
    "                            newState=self.takeAction(state,action)\n",
    "                            # if the reward_type is 0, implement the reward policy where only arriving at terminal\n",
    "                            # state will give award and the gamma is non zero\n",
    "                            if(self.reward_type==0):\n",
    "                                if(self.checkGoal(newState)):\n",
    "                                    sr_probs_dict[(newState,1.0)]=1.0\n",
    "                                else:\n",
    "                                    sr_probs_dict[(newState,0.0)]=1.0\n",
    "                            # if rewardtype is 1, implement the reward policy with no discounting factor but with a\n",
    "                            # negative reward for any step not into the terminal state and a big positive reward for\n",
    "                            # arriving at terminal state\n",
    "                            else:\n",
    "                                if(self.checkGoal(newState)):\n",
    "                                    sr_probs_dict[(newState,100.0)]=1.0\n",
    "                                else:\n",
    "                                    sr_probs_dict[(newState,-1.0)]=1.0                             \n",
    "                        d1[action]=Categorical(sr_probs_dict)\n",
    "                    d[state]=d1\n",
    "        return d\n",
    "\n",
    "SPACE = 'SPACE'\n",
    "BLOCK = 'BLOCK'\n",
    "GOAL = 'GOAL'\n",
    "maze_grid = {(0, 0): SPACE, (0, 1): BLOCK, (0, 2): SPACE, (0, 3): SPACE, (0, 4): SPACE, \n",
    "             (0, 5): SPACE, (0, 6): SPACE, (0, 7): SPACE, (1, 0): SPACE, (1, 1): BLOCK,\n",
    "             (1, 2): BLOCK, (1, 3): SPACE, (1, 4): BLOCK, (1, 5): BLOCK, (1, 6): BLOCK, \n",
    "             (1, 7): BLOCK, (2, 0): SPACE, (2, 1): BLOCK, (2, 2): SPACE, (2, 3): SPACE, \n",
    "             (2, 4): SPACE, (2, 5): SPACE, (2, 6): BLOCK, (2, 7): SPACE, (3, 0): SPACE, \n",
    "             (3, 1): SPACE, (3, 2): SPACE, (3, 3): BLOCK, (3, 4): BLOCK, (3, 5): SPACE, \n",
    "             (3, 6): BLOCK, (3, 7): SPACE, (4, 0): SPACE, (4, 1): BLOCK, (4, 2): SPACE, \n",
    "             (4, 3): BLOCK, (4, 4): SPACE, (4, 5): SPACE, (4, 6): SPACE, (4, 7): SPACE, \n",
    "             (5, 0): BLOCK, (5, 1): BLOCK, (5, 2): SPACE, (5, 3): BLOCK, (5, 4): SPACE, \n",
    "             (5, 5): BLOCK, (5, 6): SPACE, (5, 7): BLOCK, (6, 0): SPACE, (6, 1): BLOCK, \n",
    "             (6, 2): BLOCK, (6, 3): BLOCK, (6, 4): SPACE, (6, 5): BLOCK, (6, 6): SPACE, \n",
    "             (6, 7): SPACE, (7, 0): SPACE, (7, 1): SPACE, (7, 2): SPACE, (7, 3): SPACE, \n",
    "             (7, 4): SPACE, (7, 5): BLOCK, (7, 6): BLOCK, (7, 7): GOAL}\n",
    "from pprint import pprint\n",
    "user_gamma=0.9\n",
    "si_mdp1: FiniteMarkovDecisionProcess[MazeState, int] =SimpleMazeMDP(maze=maze_grid,reward_type=0)\n",
    "generator=value_iteration(si_mdp1,gamma=user_gamma)\n",
    "count=0\n",
    "for i in converge(generator,almost_equal_vfs):\n",
    "    count+=1\n",
    "print(\"number of iteration to convergence for the first reward type is\",count)\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp1, gamma=user_gamma)\n",
    "print(\"number of steps to finish maze is\",int(round(math.log(opt_vf_vi[MazeState(0,0)],user_gamma)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iteration to convergence for the second reward type is 17\n",
      "number of steps to finish maze is 16\n"
     ]
    }
   ],
   "source": [
    "si_mdp2: FiniteMarkovDecisionProcess[MazeState, int] =SimpleMazeMDP(maze=maze_grid,reward_type=1)\n",
    "generator2=value_iteration(si_mdp2,gamma=user_gamma)\n",
    "count=0\n",
    "for i in converge(generator2,almost_equal_vfs):\n",
    "    count+=1\n",
    "print(\"number of iteration to convergence for the second reward type is\",count)\n",
    "opt_vf_vi2, opt_policy_vi2 = value_iteration_result(si_mdp2, gamma=1.0)\n",
    "print(\"number of steps to finish maze is\",int(round(100-opt_vf_vi2[MazeState(0,0)]+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(start,end,policy_map):\n",
    "    path=[start]\n",
    "    while(path[-1]!=end):\n",
    "        current=path[-1]\n",
    "        action=list(policy_map[MazeState(current[0],current[1])])[0][0]\n",
    "        if(action==0):\n",
    "            path.append((current[0],current[1]-1))\n",
    "        if(action==1):\n",
    "            path.append((current[0],current[1]+1))\n",
    "        if(action==2):\n",
    "            path.append((current[0]-1,current[1]))\n",
    "        if(action==3):\n",
    "            path.append((current[0]+1,current[1]))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (2, 2), (2, 3), (2, 4), (2, 5), (3, 5), (4, 5), (4, 6), (5, 6), (6, 6), (6, 7), (7, 7)]\n",
      "[(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (2, 2), (2, 3), (2, 4), (2, 5), (3, 5), (4, 5), (4, 6), (5, 6), (6, 6), (6, 7), (7, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(get_path((0,0),(7,7),opt_policy_vi.policy_map))\n",
    "print(get_path((0,0),(7,7),opt_policy_vi2.policy_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the optimal policy path from (0,0) is printed out and they are the same for both. The number of iteration to convergence as printed out above are the same. Both have 17 iteration before convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to equation 1.2 on page 71, we can have the formulation of the value function in terms of the $\\gamma$, P and R.\n",
    "\n",
    "We know that there is an equilibrium point where:\n",
    "$$V=R+\\gamma P*V$$\n",
    "Now we want to estimate the function with $\\hat{V}$ where $\\hat{V}(s_t)=V(s_t)$ and we want to approximate if with a linear approximation with the feature matrix $\\Phi$ where $\\hat{V}=\\Phi(S)W$ where W is a m*1 weight. For the $\\hat{V}(S)$ to converge,\n",
    "we can write the update rule for the covergence.  $V(S)\\gets V(S)+\\alpha(R+\\gamma P\\hat{V}(S)-\\hat{V}(S))$ where $\\alpha$ is some learning rate parameter. For it to converge to the true value function, we need the TD error to be zero where $R+\\gamma P \\hat{V}(S)-\\hat{V}(S)=0$. So that $R+\\gamma P\\Phi(S)W-\\Phi(S)W=0$ and this is the condition for the linear feature approximation of the value function to converge to the actual value function. $\\Phi(S)W=(I-\\gamma P)^{-1}R$. So we know that $(I-\\gamma P)^{-1}R$ is in the column space of $\\Phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a)\n",
    "States: S={1,....,W}. I model the current wages as the state.\n",
    "\n",
    "Action: {(l,s),...} where l+s<=H pairs of hours spent on learning and the hour spent on searching as the action. \n",
    "\n",
    "Reward: $s_t*(H-l-s)$ reward is calculated as the wage times the number of hours to work.\n",
    "\n",
    "State transition probability: we need to take this into different cases. \n",
    "\n",
    "1. if $s_t=W$,$P(s_{t+1}=W)=1.0$ \n",
    "    with probability of 1.0, the next state will be $s_{t+1}=W$.\n",
    "\n",
    "    In this case, wage reaches maximum, cannot increase anymore.\n",
    "    \n",
    "\n",
    "2. $P(s_{t+1}=s_t+k)=poisson.pmf(k,\\alpha*l)$ for $2\\leq k<H-s_t$\n",
    "    with probability of poisson.pmf(k,alpha*l), the next state will be $s_t+k$ with $1\\leq k< H-s_t$.\n",
    "\n",
    "    In this case, you got higher wage from current company, but the wage does not reach maximum.\n",
    "    \n",
    "\n",
    "3. $P(s_{t+1}=s_t)=poisson.pmf(0,\\alpha * l) * \\beta*s/H$\n",
    "    with probability of poisson.pmf(0,alpha * l) * beta*s/H, the next state will be $s_{t+1}=s_t$.\n",
    "    \n",
    "    In this case, you didnt recieve raise and offer.\n",
    "\n",
    "4. $P(s_{t+1}=W)=1.0-poisson.cdf(W-s_t,\\alpha*l)$\n",
    "    with probability of 1.0-poisson.cdf(W-s_t,alpha*l), the next state will be $s_t=W$.\n",
    "    \n",
    "    In this case, your raise reaches maximum.\n",
    "\n",
    "5. $P(s_{t+1}=s_t+1)=poisson.pmf(0,\\alpha*l)*(\\beta*s/H)+poisson.pmf(k,\\alpha*l)$\n",
    "    with probability of poisson.pmf(0,alpha*l)*(beta*s/H), the next state will be $s_t=s_t+1$.\n",
    "\n",
    "    In this case, you recieve wage of zero or one or recieve an offer from other company.\n",
    "    \n",
    "Discount factor is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define the stateclass\n",
    "@dataclass(frozen=True)\n",
    "class SalaryState:\n",
    "    x: int\n",
    "\n",
    "# 0 is up, 1 is down, 2 is left,3 is right\n",
    "SalaryStateMapping = StateActionMapping[SalaryState,Tuple[int,int]]\n",
    "\n",
    "\n",
    "class SalaryMDP(FiniteMarkovDecisionProcess[SalaryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        H:int,\n",
    "        W:int,\n",
    "        alpha:float,\n",
    "        beta:float\n",
    "    ):\n",
    "        self.H = H\n",
    "        self.W=W\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> SalaryStateMapping:\n",
    "        d: Dict[SalaryState, Dict[Tuple[int,int], Categorical[Tuple[SalaryState,float]]]] = {}\n",
    "        for salary in range (self.W+1):\n",
    "            d1:Dict[Tuple[int,int], Categorical[Tuple[SalaryState,float]]]={}\n",
    "            if salary==self.W:\n",
    "                sr_probs_dict: Dict[Tuple[SalaryState, float], float]={}\n",
    "                sr_probs_dict[(SalaryState(salary),salary*self.H)]=1.0\n",
    "                d1[(0,0)]=Categorical(sr_probs_dict)\n",
    "            else:\n",
    "                for l in range(self.H+1):\n",
    "                    for s in range(self.H+1-l):\n",
    "                        sr_probs_dict: Dict[Tuple[SalaryState, float], float]={}\n",
    "                        reward=salary*(self.H-l-s)\n",
    "                        action=(l,s)\n",
    "                        prob_offer=self.beta*s/self.H\n",
    "                        offer_price=min(salary+1,self.W)\n",
    "                        for bonus in range(2,self.W-salary):\n",
    "                            prob_bonus=poisson.pmf(bonus,self.alpha*l)\n",
    "                            sr_probs_dict[(SalaryState(salary+bonus),reward)]=prob_bonus\n",
    "                        prob_reach_max=1.0-poisson.cdf(self.W-salary,self.alpha*l)\n",
    "                        sr_probs_dict[(SalaryState(self.W),reward)]=prob_reach_max\n",
    "                        prob_stay_zero=poisson.pmf(0,self.alpha*l)*(1-prob_offer)\n",
    "                        sr_probs_dict[(SalaryState(salary),reward)]=prob_stay_zero\n",
    "                        prob_accept_other_offer=poisson.pmf(0,self.alpha*l)*(prob_offer)+poisson.pmf(1,self.alpha*l)\n",
    "                        sr_probs_dict[(SalaryState(salary+1),reward)]=prob_accept_other_offer\n",
    "                        d1[action]=Categorical(sr_probs_dict)\n",
    "            d[SalaryState(salary)]=d1             \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdp1: FiniteMarkovDecisionProcess[SalaryState, int] =SalaryMDP(H=10,W=30,alpha=0.08,beta=0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SalaryState(x=0): 1183.7522923792844,\n",
       " SalaryState(x=1): 1259.6504926227421,\n",
       " SalaryState(x=2): 1340.415028776917,\n",
       " SalaryState(x=3): 1426.357913842387,\n",
       " SalaryState(x=4): 1517.8111660380023,\n",
       " SalaryState(x=5): 1615.1280914674717,\n",
       " SalaryState(x=6): 1718.684649031723,\n",
       " SalaryState(x=7): 1828.8809028830503,\n",
       " SalaryState(x=8): 1946.1425677166962,\n",
       " SalaryState(x=9): 2070.922650716453,\n",
       " SalaryState(x=10): 2203.703212047009,\n",
       " SalaryState(x=11): 2344.997430859798,\n",
       " SalaryState(x=12): 2495.3513213054002,\n",
       " SalaryState(x=13): 2655.325653274279,\n",
       " SalaryState(x=14): 2825.6334066075733,\n",
       " SalaryState(x=15): 3006.9962764014304,\n",
       " SalaryState(x=16): 3199.999895219283,\n",
       " SalaryState(x=17): 3399.9998886704875,\n",
       " SalaryState(x=18): 3599.999882121693,\n",
       " SalaryState(x=19): 3799.9998755728984,\n",
       " SalaryState(x=20): 3999.9998690241036,\n",
       " SalaryState(x=21): 4199.999862475308,\n",
       " SalaryState(x=22): 4399.999855926513,\n",
       " SalaryState(x=23): 4599.9998493777175,\n",
       " SalaryState(x=24): 4799.999842828925,\n",
       " SalaryState(x=25): 4999.99983628013,\n",
       " SalaryState(x=26): 5199.999829731335,\n",
       " SalaryState(x=27): 5399.999823182539,\n",
       " SalaryState(x=28): 5599.9998166337455,\n",
       " SalaryState(x=29): 5799.999810084951,\n",
       " SalaryState(x=30): 5999.999803536155}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_vf_vi,opt_policy_vi = value_iteration_result(mdp1, gamma=0.95)\n",
    "opt_vf_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SalaryState(x=0): {(10, 0): 1},\n",
       " SalaryState(x=1): {(10, 0): 1},\n",
       " SalaryState(x=2): {(10, 0): 1},\n",
       " SalaryState(x=3): {(10, 0): 1},\n",
       " SalaryState(x=4): {(10, 0): 1},\n",
       " SalaryState(x=5): {(10, 0): 1},\n",
       " SalaryState(x=6): {(10, 0): 1},\n",
       " SalaryState(x=7): {(10, 0): 1},\n",
       " SalaryState(x=8): {(10, 0): 1},\n",
       " SalaryState(x=9): {(10, 0): 1},\n",
       " SalaryState(x=10): {(10, 0): 1},\n",
       " SalaryState(x=11): {(10, 0): 1},\n",
       " SalaryState(x=12): {(10, 0): 1},\n",
       " SalaryState(x=13): {(10, 0): 1},\n",
       " SalaryState(x=14): {(0, 10): 1},\n",
       " SalaryState(x=15): {(0, 10): 1},\n",
       " SalaryState(x=16): {(0, 0): 1},\n",
       " SalaryState(x=17): {(0, 0): 1},\n",
       " SalaryState(x=18): {(0, 0): 1},\n",
       " SalaryState(x=19): {(0, 0): 1},\n",
       " SalaryState(x=20): {(0, 0): 1},\n",
       " SalaryState(x=21): {(0, 0): 1},\n",
       " SalaryState(x=22): {(0, 0): 1},\n",
       " SalaryState(x=23): {(0, 0): 1},\n",
       " SalaryState(x=24): {(0, 0): 1},\n",
       " SalaryState(x=25): {(0, 0): 1},\n",
       " SalaryState(x=26): {(0, 0): 1},\n",
       " SalaryState(x=27): {(0, 0): 1},\n",
       " SalaryState(x=28): {(0, 0): 1},\n",
       " SalaryState(x=29): {(0, 0): 1},\n",
       " SalaryState(x=30): {(0, 0): 1}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy_vi.policy_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the optimal policy we can see that for wages below 14, we should spend all the time searching for job and for wages from 14 to 15, we need to speend all time searching for job,and from 16 onward it is better to spend all time working. It is intuitive since for a higher paid job, the amount of raise from getting a new job doesnot conpensate the amount of money lost by not working for that period. For lowe paid job, the amount of raise is more valuable compared to the lost of salary for not working during the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
