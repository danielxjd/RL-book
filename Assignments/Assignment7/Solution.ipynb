{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "From Equation 6.2), wub in with $U(C_t)=log(C_t)$\n",
    "\n",
    "The HJB equation is:\n",
    "\n",
    "$$\\max_{\\pi_t,c_t}(E_t(dV^*(t,W_t))+log(c_t)*d_t)=\\rho*V^*(t,W_t)$$\n",
    "\n",
    "With Ito's lemma on $dV*$.remove $dz_t$ terms and divide by $d_t$,we get:\n",
    "$$\\max_{\\pi_t,c_t}(\\frac{dV^*}{dt}+\\frac{dV^*}{dW_t}+\\frac{d^2V^*}{dW_t^2}*\\frac{\\pi_t^2\\sigma^2W_t^2}{2}+log(c_t))=\\rho*V^*(t,W_t)$$\n",
    "sunject to terminal condition $V^*(T,W_t)=\\epsilon'log(W_T)$\n",
    "\n",
    "Using $\\Phi$ to represent the LHS to find $\\pi_t^*,c_t^*$ that maximizes $\\Phi$, we take partial derivatives.\n",
    "\n",
    "\n",
    "$$\\frac{d\\Phi}{d\\pi_t}=W_t(\\mu-r)\\frac{dV^*}{dW_t}+\\frac{d^2V^*}{dW_t^2}\\pi_t\\sigma^2W_t^2=0$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\\pi_t^*=\\frac{-\\frac{dV^*}{dW_t}(\\mu-r)}{\\frac{d^2V^2}{dW_t^2}\\sigma^2W_t}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{d\\Phi}{dc_t}=-\\frac{dV^*}{dW_t}+\\frac{1}{c_t}=0$$\n",
    "\n",
    "so we get:\n",
    "\n",
    "$$c_t^*=1/(\\frac{dV^*}{dW_t})$$\n",
    "\n",
    "Then we sub in the two expression back and solve the PDE for the final expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "State: (employment status,skill level,money) employment status is 1(employed) or 0 (unemployed)\n",
    "\n",
    "Action state: ($\\alpha_t$,$c_t$) where $\\alpha$ is the amount of time spent on learning and $c_t$ is the consumption on that day.\n",
    "\n",
    "Reward: $U(c_t)$ where U is the utility function\n",
    "\n",
    "Transition map:\n",
    "\n",
    "if employed: with action ($\\alpha_t$,$c_t$), money+=f(skill level)-c_t skill level+=g(\\alpha*T). p(employment =0)=p \n",
    "\n",
    "if unemployed: money+=-c_t, skill level=k(skill level,$\\lambda$), p(employemnt=1)=h(skill level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying out textbook codes \n",
    "\n",
    "This code take too long to execute because of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl.chapter7\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from rl.distribution import Gaussian, Sequence,Callable,Tuple\n",
    "from rl.function_approx import DNNSpec,DNNApprox,AdamGradient\n",
    "if __name__ == '__main__':\n",
    "    steps: int = 4\n",
    "    μ: float = 0.13\n",
    "    σ: float = 0.2\n",
    "    r: float = 0.07\n",
    "    a: float = 1.0\n",
    "    init_wealth: float = 1.0\n",
    "    init_wealth_var: float = 0.1\n",
    "\n",
    "    excess: float = μ - r\n",
    "    var: float = σ * σ\n",
    "    base_alloc: float = excess / (a * var)\n",
    "\n",
    "    risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "    riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "    utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "    alloc_choices: Sequence[float] = np.linspace(\n",
    "        2 / 3 * base_alloc,\n",
    "        4 / 3 * base_alloc,\n",
    "        11\n",
    "    )\n",
    "    feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "        [\n",
    "            lambda _: 1.,\n",
    "            lambda w_x: w_x[0],\n",
    "            lambda w_x: w_x[1],\n",
    "            lambda w_x: w_x[1] * w_x[1]\n",
    "        ]\n",
    "    dnn: DNNSpec = DNNSpec(\n",
    "        neurons=[],\n",
    "        bias=False,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "        output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "        output_activation_deriv=lambda y: -y\n",
    "    )\n",
    "    init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_var)\n",
    "\n",
    "    aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "        risky_return_distributions=risky_ret,\n",
    "        riskless_returns=riskless_ret,\n",
    "        utility_func=utility_function,\n",
    "        risky_alloc_choices=alloc_choices,\n",
    "        feature_functions=feature_funcs,\n",
    "        dnn_spec=dnn,\n",
    "        initial_wealth_distribution=init_wealth_distr\n",
    "    )\n",
    "\n",
    "    # vf_ff: Sequence[Callable[[float], float]] = [lambda _: 1., lambda w: w]\n",
    "    # it_vf: Iterator[Tuple[DNNApprox[float], Policy[float, float]]] = \\\n",
    "    #     aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "    # print(\"Backward Induction: VF And Policy\")\n",
    "    # print(\"---------------------------------\")\n",
    "    # print()\n",
    "    # for t, (v, p) in enumerate(it_vf):\n",
    "    #     print(f\"Time {t:d}\")\n",
    "    #     print()\n",
    "    #     opt_alloc: float = p.act(init_wealth).value\n",
    "    #     val: float = v.evaluate([init_wealth])[0]\n",
    "    #     print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    #     print(\"Weights\")\n",
    "    #     for w in v.weights:\n",
    "    #         print(w.weights)\n",
    "    #     print()\n",
    "\n",
    "    it_qvf: Iterator[DNNApprox[Tuple[float, float]]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "    print(\"Backward Induction on Q-Value Function\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print()\n",
    "    for t, q in enumerate(it_qvf):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        opt_alloc: float = max(\n",
    "            ((q.evaluate([(init_wealth, ac)])[0], ac) for ac in alloc_choices),\n",
    "            key=itemgetter(0)\n",
    "        )[1]\n",
    "        val: float = max(q.evaluate([(init_wealth, ac)])[0]\n",
    "                         for ac in alloc_choices)\n",
    "        print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(\"Optimal Weights below:\")\n",
    "        for wts in q.weights:\n",
    "            pprint(wts.weights)\n",
    "        print()\n",
    "\n",
    "    print(\"Analytical Solution\")\n",
    "    print(\"-------------------\")\n",
    "    print()\n",
    "\n",
    "    for t in range(steps):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        left: int = steps - t\n",
    "        growth: float = (1 + r) ** (left - 1)\n",
    "        alloc: float = base_alloc / growth\n",
    "        val: float = - np.exp(- excess * excess * left / (2 * var)\n",
    "                              - a * growth * (1 + r) * init_wealth) / a\n",
    "        bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
    "            np.log(np.abs(a))\n",
    "        w_t_wt: float = a * growth * (1 + r)\n",
    "        x_t_wt: float = a * excess * growth\n",
    "        x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
    "\n",
    "        print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "        print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "        print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "        print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
